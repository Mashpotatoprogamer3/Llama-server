# Llama-server
so i can run ollma
